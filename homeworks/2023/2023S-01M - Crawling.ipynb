{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c036ac2f-45f2-4cbf-918c-4e62c51ce1c5",
   "metadata": {},
   "source": [
    "# NB: This document contains master-level tasks\n",
    "\n",
    "## 1. [M][15] Account the caching policy\n",
    "\n",
    "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc23fb17-ef00-4a68-a28e-870bf586d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'public, s-maxage=31536000, max-age=604800, stale-while-revalidate=604800, stale-if-error=604800'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('https://polyfill.io/v3/polyfill.min.js').headers['Cache-Control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbf877f-f796-4046-9456-3835db86db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste here your Document class implementation from \"01 - Crawling\" file\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.download_time = 0\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "\n",
    "    def download(self):\n",
    "        #TODO download self.url content, store it in self.content and return True in case of success\n",
    "        response = requests.get(self.url)\n",
    "        if response.ok:\n",
    "            self.content = response.content\n",
    "            self.download_time = time.time()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def persist(self):\n",
    "        #TODO write document content to hard drive\n",
    "        try:\n",
    "            file = \"from_\" + self.url.split(\"/\")[2].split(\".\")[0] + \"_got_file_\" + self.url.rsplit(\"/\", 1)[1]\n",
    "            with open(file, \"wb\") as f:\n",
    "                f.write(self.content)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def load(self):\n",
    "        #TODO load content from hard drive, store it in self.content and return True in case of success\n",
    "        try:\n",
    "            file = \"from_\" + self.url.split(\"/\")[2].split(\".\")[0] + \"_got_file_\" + self.url.rsplit(\"/\", 1)[1]\n",
    "            with open(file, \"rb\") as f:\n",
    "                self.content = f.read()\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d10b7-c8ec-4a31-baff-0f9adaa5ab2d",
   "metadata": {},
   "source": [
    "Please study the documentation and implement a descendant to a `Document` class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3ade05-8bd7-48f4-bfec-0df486aaa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "class CachedDocument(Document):\n",
    "    # TODO your code here\n",
    "    def __init__(self, url):\n",
    "        Document.__init__(self, url)\n",
    "\n",
    "    def get(self):\n",
    "        response = requests.get(self.url)\n",
    "        # print(response.headers)\n",
    "        file = \"from_\" + self.url.split(\"/\")[2].split(\".\")[0] + \"_got_file_\" + self.url.rsplit(\"/\", 1)[1]\n",
    "\n",
    "        if \"cache-control\" in response.headers and re.split(\", |=\", response.headers[\"cache-control\"])[0] == \"public\":\n",
    "            max_age = int(re.split(\", |=\", response.headers[\"cache-control\"])[4])\n",
    "        else:\n",
    "            # max_age = int(re.split(\"; |= \", response.headers[\"Strict-Transport-Security\"])[1])\n",
    "            max_age = int(re.split(\", |: \", response.headers[\"NEL\"])[3])\n",
    "\n",
    "        # c_time = datetime.datetime.fromtimestamp(os.path.getctime(file)).time()\n",
    "        # cached_time = float(datetime.timedelta(hours=c_time.hour, minutes=c_time.minute, seconds=c_time.second).total_seconds())\n",
    "        cached_time = self.download_time\n",
    "        age = round(time.time() - cached_time)\n",
    "\n",
    "        if age > max_age:\n",
    "            print(\"Refreshing...\")\n",
    "            logging.info(\"Refreshing...\")\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "        else:\n",
    "            if not self.load():\n",
    "                if not self.download():\n",
    "                    raise FileNotFoundError(self.url)\n",
    "                else:\n",
    "                    self.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188adbb-f046-46ce-b1c1-8397151d027b",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "Add logging to your code and show that your code behaves differently for documents with different caching policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b917972-084f-43ea-85fd-2a932117ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing...\n",
      "Refreshing...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "doc = CachedDocument('https://polyfill.io/v3/polyfill.min.js')\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "\n",
    "doc = CachedDocument('https://yandex.ru/')\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22778c-ccde-42fe-a096-4f1978260e0a",
   "metadata": {},
   "source": [
    "## 2. [M][35] Languages\n",
    "Maybe you heard, that there are multiple languages in the world. European languages, like Russian and English, use similar puctuation, but even in this family there is ¡Spanish!\n",
    "\n",
    "Other languages can use different punctiation rules, like **Arabic or [Thai](http://www.thai-language.com/ref/breaking-words)**.\n",
    "\n",
    "Your task is to support (at least) three languages (English, Arabic, and Thai) tokenization in your `HtmlDocumentTextData` class descendant.\n",
    "\n",
    "What should you do (acceptance criteria):\n",
    "1. Use any language dection techniques, e.g. [langdetect](https://pypi.org/project/langdetect/).\n",
    "2. Use language-specific tokenization tools, e.g. for [Thai](https://pythainlp.github.io/tutorials/notebooks/pythainlp_get_started.html#Tokenization-and-Segmentation) and [Arabic](https://github.com/CAMeL-Lab/camel_tools).\n",
    "3. Use these pages to test your code: [1](https://www.bangkokair.com/tha/baggage-allowance) and [2](https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82%D8%A8%D8%A9-%D8%A8%D9%88%D8%AA%D9%8A%D9%86).\n",
    "4. Pass the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "!pip install pythainlp\n",
    "!pip install camel-tools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO extract plain text, images and links from the document\n",
    "        # self.anchors = [(\"fake link text\", \"http://fake.url/\")]\n",
    "        # self.images = [\"http://image.com/fake.jpg\"]\n",
    "        # self.text = \"fake text and some other text\"\n",
    "\n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = \" \"\n",
    "\n",
    "        page = requests.get(self.url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            url = urllib.parse.urljoin(self.url, a.get(\"href\"))\n",
    "            text = a.text\n",
    "            self.anchors.append((text, url))\n",
    "\n",
    "        for img in soup.find_all(\"img\", src=True):\n",
    "            src = urllib.parse.urljoin(self.url, img.get(\"src\"))\n",
    "            self.images.append(src)\n",
    "\n",
    "        self.text = soup.text\n",
    "        # tag = soup.body\n",
    "        # for string in tag.strings:\n",
    "        #     self.text = self.text + string.strip()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3959e7-7fd3-44e6-9ef5-840001ffdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste here your HtmlDocumentTextData class implementation from \"01 - Crawling\" file\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "    def get_sentences(self):\n",
    "        #TODO implement sentence parser\n",
    "        page = requests.get(self.url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        result = \" \"\n",
    "        tag = soup.body\n",
    "        for string in tag.strings:\n",
    "            if string.strip() == '':\n",
    "                continue\n",
    "            result += string.strip().translate({ord(k): None for k in digits})\n",
    "        return result\n",
    "\n",
    "    # def get_sentences(self):\n",
    "    #     #TODO implement sentence parser\n",
    "    #     result = []\n",
    "    #\n",
    "    #     page = requests.get(self.url)\n",
    "    #     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #     result.append(soup.text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' '))\n",
    "    #     return result\n",
    "\n",
    "    def get_word_stats(self):\n",
    "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "        sentences = self.get_sentences()\n",
    "\n",
    "        counter = Counter((x.rstrip(punctuation).lower() for y in sentences for x in y.split()))\n",
    "\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e829ea49-b334-41bf-9a6d-32cd1f046e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pythainlp\n",
    "# import camel-tools\n",
    "\n",
    "class MultilingualHtmlDocumentTextData(HtmlDocumentTextData):\n",
    "    def __init__(self, url):\n",
    "        HtmlDocumentTextData.__init__(self, url)\n",
    "\n",
    "    def get_word_stats(self):\n",
    "        sentences = self.get_sentences()\n",
    "        # print(sentences)\n",
    "\n",
    "        langs = [\"en\", \"th\", \"ar\"]\n",
    "\n",
    "        lang = detect(sentences)\n",
    "        print(lang)\n",
    "\n",
    "        words = pythainlp.word_tokenize(sentences, keep_whitespace=False)\n",
    "        counter = Counter((x.rstrip(punctuation).lower() for y in words for x in y.split()))\n",
    "        return counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878c7fe-727d-4f6e-a49a-ff5e79093d89",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc116243-9e82-4de5-8167-be862740e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "[('', 60), ('สัมภาระ', 33), ('การ', 21), ('กิโลกรัม', 21), ('ของ', 20), ('และ', 19), ('ที่', 19), ('เรา', 18), ('เที่ยวบิน', 16), ('เดินทาง', 16)]\n",
      "ar\n",
      "[('', 11), ('في', 3), ('وفاة', 3), ('الشيخة', 3), ('مريم', 3), ('و', 3), ('حميد', 2), ('بن', 2), ('واجب', 2), ('العزاء', 2)]\n"
     ]
    }
   ],
   "source": [
    "doc = MultilingualHtmlDocumentTextData(\"https://www.bangkokair.com/tha/baggage-allowance\")\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "\n",
    "doc = MultilingualHtmlDocumentTextData(\"https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\")\n",
    "print(doc.get_word_stats().most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
